{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import swingUp\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code from the previous assignment for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnQ(pt.nn.Module):\n",
    "    \"\"\"\n",
    "    Here is a basic neural network with for representing a policy \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers):\n",
    "        super().__init__()\n",
    "        \n",
    "        InputLayer = [pt.nn.Linear(stateDim+numActions,numHiddenUnits),\n",
    "                      pt.nn.ReLU()]\n",
    "        \n",
    "        HiddenLayers = []\n",
    "        for _ in range(numLayers-1):\n",
    "            HiddenLayers.append(pt.nn.Linear(numHiddenUnits,numHiddenUnits))\n",
    "            HiddenLayers.append(pt.nn.ReLU())\n",
    "            \n",
    "        \n",
    "        OutputLayer = [pt.nn.Linear(numHiddenUnits,1)]\n",
    "        \n",
    "        AllLayers = InputLayer + HiddenLayers + OutputLayer\n",
    "        self.net = pt.nn.Sequential(*AllLayers)\n",
    "        \n",
    "        self.numActions = numActions\n",
    "        \n",
    "    def forward(self,x,a):\n",
    "        x = pt.tensor(x,dtype=pt.float32)\n",
    "\n",
    "        b = pt.nn.functional.one_hot(pt.tensor(a),self.numActions)\n",
    "        \n",
    "        c = b.float().detach()\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            y = pt.cat([x,c])\n",
    "        else:\n",
    "            y = pt.cat([x.T,c.T]).T\n",
    "        return self.net(y)\n",
    "        \n",
    "    \n",
    "class deepQagent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,epsilon=.1,gamma=.9,alpha=.1,\n",
    "                c = 100,batch_size=20):\n",
    "        self.Q = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.Q_target = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.numActions = numActions\n",
    "        \n",
    "        self.S = []\n",
    "        self.A = []\n",
    "        self.S_next = []\n",
    "        self.R = []\n",
    "        self.Done = []\n",
    "        self.batch_size = batch_size\n",
    "        self.c = c\n",
    "        self.optimizer = pt.optim.SGD(self.Q.parameters(),lr=alpha)\n",
    "        self.counter = 0\n",
    "    \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon:\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        self.counter += 1\n",
    "        \n",
    "        self.S.append(s)\n",
    "        self.A.append(a)\n",
    "        self.R.append(r)\n",
    "        self.S_next.append(s_next)\n",
    "        self.Done.append(done)\n",
    "\n",
    "        B_ind = np.array(rnd.choice(len(self.S),size=self.batch_size),dtype=int)\n",
    "\n",
    "        S = np.array([self.S[j] for j in B_ind])\n",
    "        A = np.array([self.A[j] for j in B_ind])\n",
    "        R = pt.tensor(np.array([self.R[j] for j in B_ind]))\n",
    "        S_next = np.array([self.S_next[j] for j in B_ind])\n",
    "        Done = np.array([self.Done[j] for j in B_ind])\n",
    "        \n",
    "        Q_cur = self.Q(S,A).squeeze()\n",
    "\n",
    "        #B_ind = [-1]\n",
    "\n",
    "        AllActions = np.arange(self.numActions)\n",
    "        Q_next = []\n",
    "        for s_next_j,done_j in zip(S_next,Done):\n",
    "            if done_j:\n",
    "                Q_next.append(0.)\n",
    "            else:\n",
    "                Q_all = self.Q_target(np.tile(s_next_j,(self.numActions,1)),AllActions)\n",
    "                Q_next.append(pt.max(Q_all).detach().numpy())\n",
    "        Q_next = pt.tensor(np.array(Q_next),dtype=pt.float)\n",
    "        loss = .5*pt.mean((R+self.gamma * Q_next-Q_cur)**2)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "      \n",
    "        if (self.counter % self.c) == 0:\n",
    "            for p, p_target in zip(self.Q.parameters(),self.Q_target.parameters()):\n",
    "                p_target.data = p.data.clone().detach()\n",
    "                \n",
    "            \n",
    "class sarsaAgent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,\n",
    "                epsilon=.1,gamma=.9,alpha=.1):\n",
    "        self.Q = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.numActions = numActions\n",
    "        self.s_last = None\n",
    "        \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon:\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        \n",
    "        # Compute the TD error, if there is enough data\n",
    "        update = True\n",
    "        if done:\n",
    "            Q_cur = self.Q(s,a).detach().numpy()[0]\n",
    "            delta = r - Q_cur\n",
    "            self.s_last = None\n",
    "            Q_diff = self.Q(s,a)\n",
    "        elif self.s_last is not None:\n",
    "            Q_next = self.Q(s,a).detach().numpy()[0]\n",
    "            Q_cur = self.Q(self.s_last,self.a_last).detach().numpy()[0]\n",
    "            delta = self.r_last + self.gamma * Q_next - Q_cur\n",
    "            Q_diff = self.Q(self.s_last,self.a_last)\n",
    "        else:\n",
    "            update = False\n",
    "            \n",
    "        # Update the parameter via the semi-gradient method\n",
    "        if update:\n",
    "            self.Q.zero_grad()\n",
    "            Q_diff.backward()\n",
    "            for p in self.Q.parameters():\n",
    "                p.data.add_(self.alpha*delta,p.grad.data)\n",
    "                \n",
    "        if not done:\n",
    "            self.s_last = np.copy(s)\n",
    "            self.a_last = np.copy(a)\n",
    "            self.r_last = np.copy(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation is slightly modified from the previous homework. In particular, the episode lengths are restricted to be at most 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total Steps: 43 , Ave. Reward: -17.130188245128235 , Episode Length: 43 Max Up-Time: 11\n",
      "Episode: 2 Total Steps: 78 , Ave. Reward: -18.425262545945635 , Episode Length: 35 Max Up-Time: 5\n",
      "Episode: 3 Total Steps: 116 , Ave. Reward: -15.775501629212354 , Episode Length: 38 Max Up-Time: 0\n",
      "Episode: 4 Total Steps: 157 , Ave. Reward: -18.110346583744832 , Episode Length: 41 Max Up-Time: 0\n",
      "Episode: 5 Total Steps: 191 , Ave. Reward: -18.57016737294366 , Episode Length: 34 Max Up-Time: 0\n",
      "Episode: 6 Total Steps: 311 , Ave. Reward: -8.925478432004555 , Episode Length: 120 Max Up-Time: 25\n",
      "Episode: 7 Total Steps: 379 , Ave. Reward: -3.991905269797278 , Episode Length: 68 Max Up-Time: 0\n",
      "Episode: 8 Total Steps: 466 , Ave. Reward: -18.29719799071321 , Episode Length: 87 Max Up-Time: 43\n",
      "Episode: 9 Total Steps: 524 , Ave. Reward: -9.875148834595034 , Episode Length: 58 Max Up-Time: 0\n",
      "Episode: 10 Total Steps: 583 , Ave. Reward: -6.43368539353287 , Episode Length: 59 Max Up-Time: 0\n",
      "Episode: 11 Total Steps: 659 , Ave. Reward: -2.90809050165804 , Episode Length: 76 Max Up-Time: 25\n",
      "Episode: 12 Total Steps: 731 , Ave. Reward: -5.328896249899588 , Episode Length: 72 Max Up-Time: 21\n",
      "Episode: 13 Total Steps: 800 , Ave. Reward: -5.07611334505518 , Episode Length: 69 Max Up-Time: 27\n",
      "Episode: 14 Total Steps: 920 , Ave. Reward: -8.962002832943455 , Episode Length: 120 Max Up-Time: 35\n",
      "Episode: 15 Total Steps: 1015 , Ave. Reward: -1.1437455372806278 , Episode Length: 95 Max Up-Time: 44\n",
      "Episode: 16 Total Steps: 1149 , Ave. Reward: -7.740102827015581 , Episode Length: 134 Max Up-Time: 32\n",
      "Episode: 17 Total Steps: 1246 , Ave. Reward: -8.175337440931314 , Episode Length: 97 Max Up-Time: 35\n",
      "Episode: 18 Total Steps: 1405 , Ave. Reward: -8.605760949003248 , Episode Length: 159 Max Up-Time: 38\n",
      "Episode: 19 Total Steps: 1510 , Ave. Reward: -12.93664751051821 , Episode Length: 105 Max Up-Time: 32\n",
      "Episode: 20 Total Steps: 1561 , Ave. Reward: -14.904223018218605 , Episode Length: 51 Max Up-Time: 0\n",
      "Episode: 21 Total Steps: 1614 , Ave. Reward: -14.245341471643064 , Episode Length: 53 Max Up-Time: 0\n",
      "Episode: 22 Total Steps: 1675 , Ave. Reward: -12.5006263948561 , Episode Length: 61 Max Up-Time: 0\n",
      "Episode: 23 Total Steps: 1730 , Ave. Reward: -10.809652365391761 , Episode Length: 55 Max Up-Time: 0\n",
      "Episode: 24 Total Steps: 1780 , Ave. Reward: -15.12057064070028 , Episode Length: 50 Max Up-Time: 0\n",
      "Episode: 25 Total Steps: 1840 , Ave. Reward: -15.48148510540577 , Episode Length: 60 Max Up-Time: 0\n",
      "Episode: 26 Total Steps: 1942 , Ave. Reward: -9.67802455581688 , Episode Length: 102 Max Up-Time: 32\n",
      "Episode: 27 Total Steps: 2060 , Ave. Reward: -4.758972870858825 , Episode Length: 118 Max Up-Time: 23\n",
      "Episode: 28 Total Steps: 2849 , Ave. Reward: -1.0918747593992382 , Episode Length: 789 Max Up-Time: 48\n",
      "Episode: 29 Total Steps: 2961 , Ave. Reward: -9.19451628502845 , Episode Length: 112 Max Up-Time: 39\n",
      "Episode: 30 Total Steps: 3274 , Ave. Reward: -3.0918423471091874 , Episode Length: 313 Max Up-Time: 37\n",
      "Episode: 31 Total Steps: 3572 , Ave. Reward: -3.622886164449723 , Episode Length: 298 Max Up-Time: 29\n",
      "Episode: 32 Total Steps: 4042 , Ave. Reward: -0.20130238224447272 , Episode Length: 470 Max Up-Time: 90\n",
      "Episode: 33 Total Steps: 4266 , Ave. Reward: -4.193027055364144 , Episode Length: 224 Max Up-Time: 34\n",
      "Episode: 34 Total Steps: 5533 , Ave. Reward: -0.3828016107493649 , Episode Length: 1267 Max Up-Time: 81\n",
      "Episode: 35 Total Steps: 6174 , Ave. Reward: -0.12200130250758429 , Episode Length: 641 Max Up-Time: 81\n",
      "Episode: 36 Total Steps: 6524 , Ave. Reward: -3.195083096136769 , Episode Length: 350 Max Up-Time: 29\n",
      "Episode: 37 Total Steps: 6704 , Ave. Reward: -5.62012464764669 , Episode Length: 180 Max Up-Time: 48\n",
      "Episode: 38 Total Steps: 6798 , Ave. Reward: -11.555019249815436 , Episode Length: 94 Max Up-Time: 36\n",
      "Episode: 39 Total Steps: 6922 , Ave. Reward: -3.3245816015480454 , Episode Length: 124 Max Up-Time: 50\n",
      "Episode: 40 Total Steps: 7283 , Ave. Reward: -2.469299528048727 , Episode Length: 361 Max Up-Time: 66\n",
      "Episode: 41 Total Steps: 7392 , Ave. Reward: -9.578740148270606 , Episode Length: 109 Max Up-Time: 28\n",
      "Episode: 42 Total Steps: 7610 , Ave. Reward: -1.3021943212528315 , Episode Length: 218 Max Up-Time: 56\n",
      "Episode: 43 Total Steps: 8007 , Ave. Reward: -2.0440193197055403 , Episode Length: 397 Max Up-Time: 102\n",
      "Episode: 44 Total Steps: 8493 , Ave. Reward: -0.6443944010487487 , Episode Length: 486 Max Up-Time: 29\n",
      "Episode: 45 Total Steps: 8578 , Ave. Reward: -10.988313708118248 , Episode Length: 85 Max Up-Time: 23\n",
      "Episode: 46 Total Steps: 8711 , Ave. Reward: -1.812933276601247 , Episode Length: 133 Max Up-Time: 39\n",
      "Episode: 47 Total Steps: 8765 , Ave. Reward: -13.760050124556852 , Episode Length: 54 Max Up-Time: 0\n",
      "Episode: 48 Total Steps: 8831 , Ave. Reward: -6.1734001833501715 , Episode Length: 66 Max Up-Time: 0\n",
      "Episode: 49 Total Steps: 9178 , Ave. Reward: -3.081316329233488 , Episode Length: 347 Max Up-Time: 32\n",
      "Episode: 50 Total Steps: 9233 , Ave. Reward: -12.472870001888543 , Episode Length: 55 Max Up-Time: 0\n",
      "Episode: 51 Total Steps: 9289 , Ave. Reward: -10.513848234790125 , Episode Length: 56 Max Up-Time: 0\n",
      "Episode: 52 Total Steps: 9348 , Ave. Reward: -13.251854239430179 , Episode Length: 59 Max Up-Time: 0\n",
      "Episode: 53 Total Steps: 9402 , Ave. Reward: -12.439164579008473 , Episode Length: 54 Max Up-Time: 0\n",
      "Episode: 54 Total Steps: 9552 , Ave. Reward: -9.108667713758544 , Episode Length: 150 Max Up-Time: 37\n",
      "Episode: 55 Total Steps: 9606 , Ave. Reward: -14.21584282827501 , Episode Length: 54 Max Up-Time: 0\n",
      "Episode: 56 Total Steps: 9668 , Ave. Reward: -11.019076114407433 , Episode Length: 62 Max Up-Time: 18\n",
      "Episode: 57 Total Steps: 9896 , Ave. Reward: -1.6899462755036532 , Episode Length: 228 Max Up-Time: 52\n",
      "Episode: 58 Total Steps: 10011 , Ave. Reward: -5.176141599643508 , Episode Length: 115 Max Up-Time: 37\n",
      "Episode: 59 Total Steps: 10702 , Ave. Reward: -1.4251357124820812 , Episode Length: 691 Max Up-Time: 36\n",
      "Episode: 60 Total Steps: 10982 , Ave. Reward: -2.7941043038449815 , Episode Length: 280 Max Up-Time: 26\n",
      "Episode: 61 Total Steps: 11130 , Ave. Reward: -8.98239877557789 , Episode Length: 148 Max Up-Time: 32\n",
      "Episode: 62 Total Steps: 11272 , Ave. Reward: -9.55367351250381 , Episode Length: 142 Max Up-Time: 43\n",
      "Episode: 63 Total Steps: 11902 , Ave. Reward: -1.3408595914777717 , Episode Length: 630 Max Up-Time: 21\n",
      "Episode: 64 Total Steps: 12065 , Ave. Reward: -6.17142954093836 , Episode Length: 163 Max Up-Time: 30\n",
      "Episode: 65 Total Steps: 12679 , Ave. Reward: -1.095391082817434 , Episode Length: 614 Max Up-Time: 24\n",
      "Episode: 66 Total Steps: 12854 , Ave. Reward: -1.9890926031131224 , Episode Length: 175 Max Up-Time: 27\n",
      "Episode: 67 Total Steps: 13241 , Ave. Reward: -0.9162949614562396 , Episode Length: 387 Max Up-Time: 31\n",
      "Episode: 68 Total Steps: 13369 , Ave. Reward: -6.213198699973949 , Episode Length: 128 Max Up-Time: 38\n",
      "Episode: 69 Total Steps: 13500 , Ave. Reward: -5.450510850668865 , Episode Length: 131 Max Up-Time: 31\n",
      "Episode: 70 Total Steps: 13672 , Ave. Reward: -4.413719837123894 , Episode Length: 172 Max Up-Time: 34\n",
      "Episode: 71 Total Steps: 13855 , Ave. Reward: -2.4064495403137443 , Episode Length: 183 Max Up-Time: 28\n",
      "Episode: 72 Total Steps: 13906 , Ave. Reward: -16.441821174057797 , Episode Length: 51 Max Up-Time: 0\n",
      "Episode: 73 Total Steps: 14326 , Ave. Reward: -1.2403891249712382 , Episode Length: 420 Max Up-Time: 45\n",
      "Episode: 74 Total Steps: 14470 , Ave. Reward: -5.939770361448062 , Episode Length: 144 Max Up-Time: 36\n",
      "Episode: 75 Total Steps: 14994 , Ave. Reward: -0.8033109280466576 , Episode Length: 524 Max Up-Time: 32\n",
      "Episode: 76 Total Steps: 15083 , Ave. Reward: -11.901148004017168 , Episode Length: 89 Max Up-Time: 26\n",
      "Episode: 77 Total Steps: 15205 , Ave. Reward: -7.511892980697937 , Episode Length: 122 Max Up-Time: 38\n",
      "Episode: 78 Total Steps: 15522 , Ave. Reward: -3.3480869772266466 , Episode Length: 317 Max Up-Time: 49\n",
      "Episode: 79 Total Steps: 15676 , Ave. Reward: -8.536906311996606 , Episode Length: 154 Max Up-Time: 38\n",
      "Episode: 80 Total Steps: 15883 , Ave. Reward: -3.422129399393156 , Episode Length: 207 Max Up-Time: 42\n",
      "Episode: 81 Total Steps: 16056 , Ave. Reward: -8.207647374873664 , Episode Length: 173 Max Up-Time: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 82 Total Steps: 16648 , Ave. Reward: 0.0210803532373115 , Episode Length: 592 Max Up-Time: 61\n",
      "Episode: 83 Total Steps: 16804 , Ave. Reward: -7.376338648083383 , Episode Length: 156 Max Up-Time: 29\n",
      "Episode: 84 Total Steps: 17055 , Ave. Reward: -5.1435106066078395 , Episode Length: 251 Max Up-Time: 137\n",
      "Episode: 85 Total Steps: 17182 , Ave. Reward: -4.111810857494476 , Episode Length: 127 Max Up-Time: 44\n",
      "Episode: 86 Total Steps: 17438 , Ave. Reward: -4.847483506003263 , Episode Length: 256 Max Up-Time: 33\n",
      "Episode: 87 Total Steps: 17639 , Ave. Reward: -4.668471383795126 , Episode Length: 201 Max Up-Time: 60\n",
      "Episode: 88 Total Steps: 17792 , Ave. Reward: -8.367913631520913 , Episode Length: 153 Max Up-Time: 36\n",
      "Episode: 89 Total Steps: 17952 , Ave. Reward: -8.011487171534863 , Episode Length: 160 Max Up-Time: 37\n",
      "Episode: 90 Total Steps: 18098 , Ave. Reward: -8.28355770358278 , Episode Length: 146 Max Up-Time: 40\n",
      "Episode: 91 Total Steps: 18228 , Ave. Reward: -1.1730693901201468 , Episode Length: 130 Max Up-Time: 29\n",
      "Episode: 92 Total Steps: 18355 , Ave. Reward: -3.204775454455646 , Episode Length: 127 Max Up-Time: 31\n",
      "Episode: 93 Total Steps: 18633 , Ave. Reward: -2.5432143880427196 , Episode Length: 278 Max Up-Time: 35\n",
      "Episode: 94 Total Steps: 19103 , Ave. Reward: -0.4872915502002759 , Episode Length: 470 Max Up-Time: 49\n",
      "Episode: 95 Total Steps: 19723 , Ave. Reward: -1.676373557445751 , Episode Length: 620 Max Up-Time: 107\n",
      "Episode: 96 Total Steps: 20389 , Ave. Reward: -0.037407847941538064 , Episode Length: 666 Max Up-Time: 61\n",
      "Episode: 97 Total Steps: 20568 , Ave. Reward: -0.6492053301051517 , Episode Length: 179 Max Up-Time: 0\n",
      "Episode: 98 Total Steps: 21557 , Ave. Reward: 0.2634518883079127 , Episode Length: 989 Max Up-Time: 97\n",
      "Episode: 99 Total Steps: 22947 , Ave. Reward: -0.2601052200100389 , Episode Length: 1390 Max Up-Time: 61\n",
      "Episode: 100 Total Steps: 23836 , Ave. Reward: 0.08976935165795796 , Episode Length: 889 Max Up-Time: 73\n",
      "Episode: 101 Total Steps: 23940 , Ave. Reward: -11.007747617014472 , Episode Length: 104 Max Up-Time: 23\n",
      "Episode: 102 Total Steps: 24013 , Ave. Reward: -5.924248753727096 , Episode Length: 73 Max Up-Time: 1\n",
      "Episode: 103 Total Steps: 25022 , Ave. Reward: -0.19668093138113518 , Episode Length: 1009 Max Up-Time: 87\n",
      "Episode: 104 Total Steps: 25120 , Ave. Reward: -11.354066879589295 , Episode Length: 98 Max Up-Time: 24\n",
      "Episode: 105 Total Steps: 25309 , Ave. Reward: -0.9671895199243252 , Episode Length: 189 Max Up-Time: 37\n",
      "Episode: 106 Total Steps: 25755 , Ave. Reward: -0.4348058987272664 , Episode Length: 446 Max Up-Time: 54\n",
      "Episode: 107 Total Steps: 26221 , Ave. Reward: -2.45919373610662 , Episode Length: 466 Max Up-Time: 49\n",
      "Episode: 108 Total Steps: 26480 , Ave. Reward: -0.7552666864363455 , Episode Length: 259 Max Up-Time: 34\n",
      "Episode: 109 Total Steps: 26878 , Ave. Reward: -1.3079903939396764 , Episode Length: 398 Max Up-Time: 56\n",
      "Episode: 110 Total Steps: 27044 , Ave. Reward: -1.919716759251901 , Episode Length: 166 Max Up-Time: 34\n",
      "Episode: 111 Total Steps: 28405 , Ave. Reward: -0.5354002679365046 , Episode Length: 1361 Max Up-Time: 65\n",
      "Episode: 112 Total Steps: 28585 , Ave. Reward: -2.5332721482935656 , Episode Length: 180 Max Up-Time: 49\n",
      "Episode: 113 Total Steps: 28785 , Ave. Reward: -1.9880923718306183 , Episode Length: 200 Max Up-Time: 38\n",
      "Episode: 114 Total Steps: 29390 , Ave. Reward: -0.6493221999905787 , Episode Length: 605 Max Up-Time: 61\n",
      "Episode: 115 Total Steps: 30599 , Ave. Reward: -0.4146544666684754 , Episode Length: 1209 Max Up-Time: 87\n",
      "Episode: 116 Total Steps: 30721 , Ave. Reward: -3.2985041239139674 , Episode Length: 122 Max Up-Time: 18\n",
      "Episode: 117 Total Steps: 32095 , Ave. Reward: 0.036237499742865 , Episode Length: 1374 Max Up-Time: 69\n",
      "Episode: 118 Total Steps: 32543 , Ave. Reward: -0.43815458764072135 , Episode Length: 448 Max Up-Time: 57\n",
      "Episode: 119 Total Steps: 32833 , Ave. Reward: -2.2159823697815275 , Episode Length: 290 Max Up-Time: 106\n",
      "Episode: 120 Total Steps: 33031 , Ave. Reward: 0.07188111210791182 , Episode Length: 198 Max Up-Time: 39\n",
      "Episode: 121 Total Steps: 33189 , Ave. Reward: -3.640983245219314 , Episode Length: 158 Max Up-Time: 54\n",
      "Episode: 122 Total Steps: 33333 , Ave. Reward: -3.5779596695887563 , Episode Length: 144 Max Up-Time: 48\n",
      "Episode: 123 Total Steps: 33492 , Ave. Reward: -1.8538289546706435 , Episode Length: 159 Max Up-Time: 53\n",
      "Episode: 124 Total Steps: 34147 , Ave. Reward: -1.0305218537192817 , Episode Length: 655 Max Up-Time: 45\n",
      "Episode: 125 Total Steps: 34299 , Ave. Reward: -5.652805473031929 , Episode Length: 152 Max Up-Time: 47\n",
      "Episode: 126 Total Steps: 35542 , Ave. Reward: -0.07172394298202667 , Episode Length: 1243 Max Up-Time: 62\n",
      "Episode: 127 Total Steps: 36468 , Ave. Reward: 0.04183032732445591 , Episode Length: 926 Max Up-Time: 106\n",
      "Episode: 128 Total Steps: 36615 , Ave. Reward: -8.424659057281042 , Episode Length: 147 Max Up-Time: 35\n",
      "Episode: 129 Total Steps: 37465 , Ave. Reward: -0.8704934572534635 , Episode Length: 850 Max Up-Time: 108\n",
      "Episode: 130 Total Steps: 39089 , Ave. Reward: 0.060868938341345324 , Episode Length: 1624 Max Up-Time: 82\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ae40664868ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_next\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3ca1572a4107>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, r, s_next, done)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mQ_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mQ_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_next_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumActions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAllActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mQ_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mQ_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3ca1572a4107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, a)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumActions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is the environment\n",
    "env = swingUp.SwingUpEnv()\n",
    "\n",
    "# For simplicity, we only consider forces of -1 and 1\n",
    "numActions = 2\n",
    "Actions = np.linspace(-1,1,numActions)\n",
    "\n",
    "# This is our learning agent\n",
    "gamma = .95\n",
    "agent = deepQagent(5,numActions,20,2,epsilon=1e-1,gamma=gamma,batch_size=50,\n",
    "                   c= 100,alpha=1e-4)\n",
    "\n",
    "#agent = sarsaAgent(5,numActions,20,2,epsilon=5e-2,gamma=gamma,alpha=1e-4)\n",
    "maxSteps = 5e5\n",
    "\n",
    "# This is a helper to deal with the fact that x[2] is actually an angle\n",
    "x_to_y = lambda x : np.array([x[0],x[1],np.cos(x[2]),np.sin(x[2]),x[3]])\n",
    "\n",
    "R = []\n",
    "UpTime = []\n",
    "\n",
    "step = 0\n",
    "ep = 0\n",
    "maxLen = 500\n",
    "while step < maxSteps:\n",
    "    ep += 1\n",
    "    x = env.reset()\n",
    "    C = 0.\n",
    "    \n",
    "    done = False\n",
    "    t = 1\n",
    "    while not done:\n",
    "        t += 1\n",
    "        step += 1\n",
    "        y = x_to_y(x)\n",
    "        a = agent.action(y)\n",
    "        u = Actions[a:a+1]\n",
    "        env.render()\n",
    "        x_next,c,done,info = env.step(u)\n",
    "        \n",
    "        max_up_time = info['max_up_time']\n",
    "        y_next = x_to_y(x_next)\n",
    "\n",
    "        C += (1./t)*(c-C)\n",
    "        agent.update(y,a,c,y_next,done)\n",
    "        x = np.copy(x_next)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if step >= maxSteps:\n",
    "            break\n",
    "            \n",
    "        #if t > maxLen:\n",
    "        #    agent.s_last = None\n",
    "        #    break\n",
    "            \n",
    "        \n",
    "        R.append(C)\n",
    "    UpTime.append(max_up_time)\n",
    "    #print('t:',ep+1,', R:',C,', L:',t-1,', G:',G,', Q:', Q_est, 'U:', max_up_time)\n",
    "    print('Episode:',ep,'Total Steps:',step,', Ave. Reward:',C,', Episode Length:',t-1, 'Max Up-Time:', max_up_time)\n",
    "env.close()\n",
    "\n",
    "plt.plot(UpTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question \n",
    "\n",
    "Implement deep Q-learning as described in the paper here:\n",
    "\n",
    "https://daiwk.github.io/assets/dqn.pdf\n",
    "\n",
    "In this paper, we have the states, and so there is no need to do the pre-processing described there.\n",
    "\n",
    "In my tests on this problem, it works substantially better than the SARSA  implementation \n",
    "with the following design choices:\n",
    "* Use the same Q-network architecture as used  in the SARSA algorithm\n",
    "* Same step size, discount factor, and learning rate as above\n",
    "* Mini-batch size of 20\n",
    "* Update the target network every 100 steps\n",
    "\n",
    "The deep Q-learning method can be implemented via a modification of the SARSA code above.\n",
    "\n",
    "You could probably make it work even better with further tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this code below and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
