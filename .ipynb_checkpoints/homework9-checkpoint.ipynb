{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import swingUp\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code from the previous assignment for convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnQ(pt.nn.Module):\n",
    "    \"\"\"\n",
    "    Here is a basic neural network with for representing a policy \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers):\n",
    "        super().__init__()\n",
    "        \n",
    "        InputLayer = [pt.nn.Linear(stateDim+numActions,numHiddenUnits),\n",
    "                      pt.nn.ReLU()] \n",
    "        \n",
    "        HiddenLayers = []\n",
    "        for _ in range(numLayers-1):\n",
    "            HiddenLayers.append(pt.nn.Linear(numHiddenUnits,numHiddenUnits))\n",
    "            HiddenLayers.append(pt.nn.ReLU())\n",
    "            \n",
    "        \n",
    "        OutputLayer = [pt.nn.Linear(numHiddenUnits,1)]\n",
    "        \n",
    "        AllLayers = InputLayer + HiddenLayers + OutputLayer\n",
    "        self.net = pt.nn.Sequential(*AllLayers)\n",
    "        \n",
    "        self.numActions = numActions\n",
    "        \n",
    "    def forward(self,x,a):\n",
    "        x = pt.tensor(x,dtype=pt.float32)\n",
    "        a = pt.tensor(a,dtype=pt.int64)\n",
    "        b = pt.nn.functional.one_hot(a,self.numActions)\n",
    "        \n",
    "        c = b.float().detach()\n",
    "        y = pt.cat([x,c])\n",
    "        \n",
    "        return self.net(y)\n",
    "            \n",
    "class sarsaAgent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,\n",
    "                epsilon=.1,gamma=.9,alpha=.1):\n",
    "        self.Q = nnQ(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.numActions = numActions\n",
    "        self.s_last = None\n",
    "        \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon: # If less than epsilon, t\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        \n",
    "        # Compute the TD error, if there is enough data\n",
    "        update = True\n",
    "        if done:\n",
    "            Q_cur = self.Q(s,a).detach().numpy()[0] \n",
    "            delta = r - Q_cur\n",
    "            self.s_last = None\n",
    "            Q_diff = self.Q(s,a)\n",
    "        elif self.s_last is not None: # Not terminal state\n",
    "            Q_next = self.Q(s,a).detach().numpy()[0]\n",
    "            Q_cur = self.Q(self.s_last,self.a_last).detach().numpy()[0]\n",
    "            delta = self.r_last + self.gamma * Q_next - Q_cur \n",
    "            Q_diff = self.Q(self.s_last,self.a_last)\n",
    "        else:\n",
    "            update = False\n",
    "            \n",
    "        # Update the parameter (weights) via the semi-gradient method\n",
    "        if update:\n",
    "            \n",
    "            self.Q.zero_grad()\n",
    "            Q_diff.backward()\n",
    "            for p in self.Q.parameters():\n",
    "                p.data.add_(self.alpha*delta,p.grad.data)\n",
    "                \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        if not done:\n",
    "            self.s_last = np.copy(s)\n",
    "            self.a_last = np.copy(a)\n",
    "            self.r_last = np.copy(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation is slightly modified from the previous homework. In particular, the episode lengths are restricted to be at most 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the environment\n",
    "env = swingUp.SwingUpEnv()\n",
    "\n",
    "# For simplicity, we only consider forces of -1 and 1\n",
    "numActions = 2\n",
    "Actions = np.linspace(-1,1,numActions)\n",
    "\n",
    "# This is our learning agent\n",
    "gamma = .95\n",
    "\n",
    "agent = sarsaAgent(5,numActions,20,2,epsilon=5e-2,gamma=gamma,alpha=1e-4)\n",
    "maxSteps = 2e5\n",
    "\n",
    "# This is a helper to deal with the fact that x[2] is actually an angle\n",
    "x_to_y = lambda x : np.array([x[0],x[1],np.cos(x[2]),np.sin(x[2]),x[3]])\n",
    "\n",
    "R = []\n",
    "UpTime = []\n",
    "\n",
    "step = 0\n",
    "ep = 0\n",
    "maxLen = 500\n",
    "while step < maxSteps: # Outerloop is no of steps\n",
    "    ep += 1\n",
    "    x = env.reset() # Initialize\n",
    "    C = 0.\n",
    "    \n",
    "    done = False\n",
    "    t = 1\n",
    "    while not done:  # These are the number of \n",
    "        t += 1\n",
    "        step += 1\n",
    "        y = x_to_y(x)\n",
    "        \n",
    "        # step #1\n",
    "        a = agent.action(y)\n",
    "        u = Actions[a:a+1]\n",
    "        env.render()\n",
    "        \n",
    "        # step #2\n",
    "        x_next,c,done,info = env.step(u)\n",
    "        \n",
    "        max_up_time = info['max_up_time']\n",
    "        y_next = x_to_y(x_next)\n",
    "    \n",
    "        C += (1./t)*(c-C) # avg reward\n",
    "        \n",
    "         \n",
    "        print(y)\n",
    "        agent.update(y,a,c,y_next,done)\n",
    "        x = np.copy(x_next)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if step >= maxSteps:\n",
    "            break\n",
    "            \n",
    "        if t > maxLen:\n",
    "            agent.s_last = None\n",
    "            break\n",
    "            \n",
    "        \n",
    "        R.append(C)\n",
    "    UpTime.append(max_up_time)\n",
    "    #print('t:',ep+1,', R:',C,', L:',t-1,', G:',G,', Q:', Q_est, 'U:', max_up_time)\n",
    "    print('Episode:',ep,'Total Steps:',step,', Ave. Reward:',C,', Episode Length:',t-1, 'Max Up-Time:', max_up_time)\n",
    "env.close()\n",
    "\n",
    "plt.plot(UpTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question \n",
    "\n",
    "Implement deep Q-learning as described in the paper here:\n",
    "\n",
    "https://daiwk.github.io/assets/dqn.pdf\n",
    "\n",
    "In this paper, we have the states, and so there is no need to do the pre-processing described there.\n",
    "\n",
    "In my tests on this problem, it works substantially better than the SARSA  implementation \n",
    "with the following design choices:\n",
    "* Use the same Q-network architecture as used  in the SARSA algorithm\n",
    "* Same step size, discount factor, and learning rate as above\n",
    "* Mini-batch size of 20\n",
    "* Update the target network every 100 steps\n",
    "\n",
    "The deep Q-learning method can be implemented via a modification of the SARSA code above.\n",
    "\n",
    "You could probably make it work even better with further tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this code below and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(pt.nn.Module):\n",
    "    \"\"\"\n",
    "    Here is a basic neural network with for representing a policy \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers):\n",
    "        super().__init__()\n",
    "        \n",
    "        InputLayer = [pt.nn.Linear(stateDim+numActions,numHiddenUnits),\n",
    "                      pt.nn.ReLU()] \n",
    "        \n",
    "        HiddenLayers = []\n",
    "        for _ in range(numLayers-1):\n",
    "            HiddenLayers.append(pt.nn.Linear(numHiddenUnits,numHiddenUnits))\n",
    "            HiddenLayers.append(pt.nn.ReLU())\n",
    "            \n",
    "        \n",
    "        OutputLayer = [pt.nn.Linear(numHiddenUnits,1)]\n",
    "        \n",
    "        AllLayers = InputLayer + HiddenLayers + OutputLayer\n",
    "        self.net = pt.nn.Sequential(*AllLayers)\n",
    "        \n",
    "        self.numActions = numActions\n",
    "        \n",
    "    def forward(self,x,a):\n",
    "        x = pt.tensor(x,dtype=pt.float32)\n",
    "        a = pt.tensor(a,dtype=pt.int64)\n",
    "        b = pt.nn.functional.one_hot(a,self.numActions)\n",
    "        \n",
    "        c = b.float().detach()\n",
    "        y = pt.cat([x,c])\n",
    "        \n",
    "        return self.net(y)\n",
    "            \n",
    "class DQNAgent:\n",
    "    def __init__(self,stateDim,numActions,numHiddenUnits,numLayers,\n",
    "                epsilon=.1,gamma=.9,alpha=.1):\n",
    "        self.Q = DQN(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.Q_c = DQN(stateDim,numActions,numHiddenUnits,numLayers)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.numActions = numActions\n",
    "        self.s_last = None\n",
    "        \n",
    "    def action(self,x):\n",
    "        # This is an epsilon greedy selection\n",
    "        if rnd.rand() < self.epsilon: # If less than epsilon, t\n",
    "            a = rnd.randint(numActions)\n",
    "        else:\n",
    "            qBest = -np.inf\n",
    "            for aTest in range(self.numActions):\n",
    "                qTest = self.Q(x,aTest).detach().numpy()[0]\n",
    "                if qTest > qBest:\n",
    "                    qBest = qTest\n",
    "                    a = aTest\n",
    "        return a\n",
    "    \n",
    "    def update(self,s,a,r,s_next,done):\n",
    "        update = True\n",
    "        if done:\n",
    "            y = self.r_last \n",
    "\n",
    "        elif self.s_last is not None: # Not terminal state\n",
    "            Q_next =[];\n",
    "            Q_next.append(self.Q_c(s,1).detach().numpy()[0])\n",
    "            Q_next.append(self.Q_c(s,-1).detach().numpy()[0])\n",
    "            y = self.r_last + self.gamma * np.max(Q_next) \n",
    "        else:\n",
    "            update = False\n",
    "            return y\n",
    "        \n",
    "        if not done:\n",
    "            self.s_last = np.copy(s)\n",
    "            self.a_last = np.copy(a)\n",
    "            self.r_last = np.copy(r)\n",
    "            \n",
    "    def gradUpdate(y,s_j_batch, a_batch):\n",
    "        for i in range(100):\n",
    "            loss +=  (y[i] - self.Q(s_j_batch[i],a_batch[i]))**2/100\n",
    "        loss.zero_grad()\n",
    "        loss.backward()\n",
    "        for p in self.Q.parameters():\n",
    "            p.data.add_(loss.grad.data)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the environment\n",
    "env = swingUp.SwingUpEnv()\n",
    "\n",
    "# For simplicity, we only consider forces of -1 and 1\n",
    "numActions = 2\n",
    "Actions = np.linspace(-1,1,numActions)\n",
    "\n",
    "# This is our learning agent\n",
    "gamma = .95\n",
    "agent = DQNAgent(5,numActions,20,1,epsilon=5e-2,gamma=gamma,alpha=1e-4)\n",
    "#agent_cap = DQNAgent(5,numActions,20,1,epsilon=5e-2,gamma=gamma,alpha=1e-4)\n",
    "\n",
    "maxSteps = 2e3\n",
    "minibatch_size = 20\n",
    "# This is a helper to deal with the fact that x[2] is actually an angle\n",
    "x_to_y = lambda x : np.array([x[0],x[1],np.cos(x[2]),np.sin(x[2]),x[3]]) # this is dyn of the cartpole\n",
    "\n",
    "R = []\n",
    "UpTime = []\n",
    "D = []\n",
    "step = 0\n",
    "ep = 0\n",
    "observe = 10000\n",
    "while step < maxSteps:\n",
    "    ep += 1\n",
    "    x = env.reset() # initialize s\n",
    "    C = 0.  # WHATS C\n",
    "    \n",
    "    done = False\n",
    "    t = 1\n",
    "    while not done:\n",
    "        t += 1 \n",
    "        step += 1\n",
    "        y = x_to_y(x)\n",
    "        \n",
    "        # Step 1: Pick At via epsilon-greedy \n",
    "        a = agent.action(y)\n",
    "        u = Actions[a:a+1]\n",
    "        env.render()\n",
    "        \n",
    "        # Step 2: Observe S_t+1 and R_t+1\n",
    "        x_next,c,done,info = env.step(u)\n",
    " \n",
    "        max_up_time = info['max_up_time']\n",
    "        y_next = x_to_y(x_next)\n",
    "\n",
    "        C += (1./t)*(c-C)\n",
    "        D.append((y, a, c, y_next,done))\n",
    "        if t > observe:\n",
    "\n",
    "            minibatch = random.sample(D, minibatch_size)\n",
    "            # Step 4: Select a mini batch B in D\n",
    "            minibatch_idx = np.random.randint(t, size=(minibatch_size,))\n",
    "            # get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "            term = [d[4] for d in minibatch]\n",
    "            \n",
    "            y_batch = []\n",
    "            \n",
    "            for i in range(0, len(minibatch)):\n",
    "                y_batch.append(agent.update(s_j_batch,a_batch,r_batch[i],s_j1_batch[i],done[i]))\n",
    "                \n",
    "          \n",
    "        # Compute gradient\n",
    "        agent.gradUpdate(y_batch)\n",
    "        \n",
    "        \n",
    "        x = x_next\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        if step >= maxSteps:\n",
    "            break\n",
    "            \n",
    "        \n",
    "        R.append(C)\n",
    "    UpTime.append(max_up_time)\n",
    "    #print('t:',ep+1,', R:',C,', L:',t-1,', G:',G,', Q:', Q_est, 'U:', max_up_time)\n",
    "    print('Episode:',ep,'Total Steps:',step,', Ave. Reward:',C,', Episode Length:',t-1, 'Max Up-Time:', max_up_time)\n",
    "env.close()\n",
    "\n",
    "plt.plot(UpTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
