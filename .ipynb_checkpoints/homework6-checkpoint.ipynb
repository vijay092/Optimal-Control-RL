{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oh hey\n",
    "\n",
    "This homework is just coding. We will be working with a simple example from OpenAI gym called \"Frozen Lake\". It is a text-based maze environment that your controller will learn to navigate. It is slippery, however, so sometimes you don't always move where you try to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The board can be interpretted as follows\n",
    "* S - Start State\n",
    "* F - Frozen parts\n",
    "* H - Holes\n",
    "* G - Goal\n",
    "\n",
    "The episode ends if you hit a hole or the goal state. You recieve a reward of $1$ if you reach the goal and a reward of $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "\n",
    "done = False\n",
    "R = []\n",
    "while not done:\n",
    "    a = env.action_space.sample()\n",
    "        \n",
    "    s, r, done, info = env.step(a)\n",
    "    R.append(r)\n",
    "\n",
    "    \n",
    "env.render()\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "Use on-policy Monte Carlo control to learn a policy for this problem. (See 5.4 of the book.) Simulate $100,000$ episodes. Let $G^i$ be the total reward for episode $i$. \n",
    "\n",
    "Make a plot of $\\frac{1}{i}\\sum_{j=1}^i G^j$ (the running average of the rewards), for $i=1$ to $100,000$. Try to get it to learn well, but don't spend forever tuning parameters. Go outside. Treat yourself. \n",
    "\n",
    "Here are some tips about the learning process.\n",
    "\n",
    "* With well-tuned controller, average rewards of around 0.7 are possible. (That is, it is solved 70% of the time) \n",
    "* With the basic Monte Carlo controller, you probably won't get very close to that after $100,000$ steps. (Average rewards between 0.2 and 0.4 would be more likely.)\n",
    "* The main free parameter in the Monte Carlo algorithm is $\\epsilon$, which is the probability that you choose the action completely at random. Typically $\\epsilon$ must start fairly large, to promote exploration, and then decrease as more data has been collected. A reasonble tuning rule takes the form $\\epsilon_i = i^{-\\beta}$ on episode $i$, where $\\beta\\in (0,1)$. You may need to play around with this to get good results. \n",
    "* The performance of the learning algorithm can vary from trial to trial due to randomness.\n",
    "* If an episode is taking a long time, the environment will time out. When that happens, the `info` dictionary will have an entry with key 'TimeLimit.truncated'. This is not a big deal for the Monte Carlo method. It will be important when we start doing policy iteration on the estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your simulation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "Now design a controller that learns a model for the transition probabilities, $p(s'|s,a)$ and the rewards $r(s,a) = \\mathbb{E}[R_{t+1}|S_t = s,A_t = a]$, and uses the estimates for policy iterations.\n",
    "\n",
    "Specifically, do the following. Fix some initial policy. Then repeat the following procedure for 100,000 (or fewer if your code is very slow) steps:\n",
    "\n",
    "* Simulate an episode with the current policy.\n",
    "* Use the new data to update your model of $p(s'|s,a)$ and $r(s,a)$.\n",
    "* Compute an updated policy by policy iteration.\n",
    "* Modify the policy to make it $\\epsilon$-greedy.\n",
    "\n",
    "Some tips:\n",
    "* For policy iteration to work properly, you need to ensure that $p(\\hat s | \\hat s,a)=1$ and $r(\\hat s,a)=0$ for all terminal states, $\\hat s$. \n",
    "* You can check that a state is terminal if it is the state returned by the `env.step` function when the episode ends normally. That is when `done` is `True` and `info['TimeLimit.truncated']` is not `True`.\n",
    "* When the episode ends normally, `'TimeLimit.truncated'` will not be a key of the `info` dictionary.\n",
    "* You will need to use a discount factor $0 < \\gamma < 1$  to ensure tht policy evaluation is solvable.\n",
    "* The matrix vector version of policy evaluation from the last will likely be more efficient than the iteration described in the book.\n",
    "* You will still need to decrease $\\epsilon$ as the number of episodes increases as above.\n",
    "* This strategy should improve more quickly (as a function of the episode count) than the basic Monte Carlo method, at the expense of extra computation.\n",
    "* There is a good chance you won't be able to get up to rewards in the 0.7 range without tuning $\\epsilon$ very well or running a very large number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your simulation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
