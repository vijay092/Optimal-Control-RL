{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oh hey\n",
    "\n",
    "This homework is just coding. We will code up $Q$-learning for the frozen lake problem and then see a simple, exact method for doing LQR from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "env=gym.make('FrozenLake-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "Use Q-learning to learn a policy for this problem. Simulate $100,000$ episodes. Let $G^i$ be the total reward for episode $i$. \n",
    "\n",
    "Make a plot of $\\frac{1}{i}\\sum_{j=1}^i G^j$ (the running average of the rewards), for $i=1$ to $100,000$. \n",
    "\n",
    "As with the other algorithms, getting high rewards may take a lot of fine tuning. Just like last week, you'll probably need to tune the exploration parameter, $\\epsilon$. Additionally, you'll need to tune the step size parameter, $\\alpha$. Try to get it to learn well, but don't spend forever tuning parameters. Go outside. Treat yourself. \n",
    "\n",
    "You will probably need to use a discount factor of $\\gamma <1$ to make it work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your simulation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An exact optimal gain from measurements\n",
    "\n",
    "In class on Friday, Oct. 25, we discussed a paper by Bradtke, Ydstie, and Barto. It is posted on the canvas for reference.\n",
    "\n",
    "This work assumes that the initial controller is stabilizing. This is a major limitation of this and many works on reinforcement learning, especially for continuous systems, and LQR in particular.\n",
    "\n",
    "In this problem, we will give a very simple method to compute a stabilizing controller from trajectory data. \n",
    "\n",
    "We consider a problem of the form:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\min && \\sum_{t=0}^{\\infty} (x_t^\\top Q x_t + u_t^\\top R u_t) dt \\\\\n",
    "& \\textrm{subject to} && x_{t+1} = Ax_t + Bu_t,\n",
    "\\end{align*}\n",
    "where $(A,B,Q,R)$ are unknown matrices. Assume that $x_t \\in \\mathbb{R}^n$ and $u_t \\in \\mathbb{R}^p$.\n",
    "\n",
    "We assume that we can we can observe the state and the cost, put in arbitrary inputs, and restart the system if needed. (This is important for unstable systems, which could grow too large and cause numerical problems.)\n",
    "\n",
    "The procedure works as follows:\n",
    "\n",
    "* Generate $N=(n+p)(n+p+1)$ samples $(x_i,u_i,c_i,x_i^+)$ for $i=0,\\ldots,N-1$, where $x_i^+ = A x_i + B u_i$ and $c_i = x_i^\\top Q x_i + u_i^\\top R u_i$. A reasonable procedure for doing this is as follows:\n",
    "    * Fix an upper bound, $M$, on the size of $\\|x\\|$. \n",
    "    * Generate $x_0$ randomly and ensure that $\\|x_i\\| \\le M$.\n",
    "    * For $i \\ge 0$ assume that the current state $x_i$ with $\\|x_i\\|\\le M$ has been generated already.\n",
    "        * Generate $u_i$ randomly and apply it to the system.\n",
    "        * Measure the next state, $x_i^+$ and cost, $c_i$.\n",
    "        * If $\\|x_i^+\\| \\le M$, set $x_{i+1} = x_i^+$. Otherwise, set $x_{i+1}$ randomly and ensure that $\\|x_i\\|\\le M$. \n",
    "* Compute $A$ and $B$ via least-squares, based on the system of equations $x_i^+ = Ax_i + Bu_i$ for $i=0,\\ldots, N-1$.\n",
    "* Compute $Q$ and $R$ vial least-squares, based on the system of equations $c_i = x_i^\\top Q x_i + u_i^\\top R u_i$. (Using the reshaping trick from the paper is useful for this.)\n",
    "* Compute the optimal gain via the discrete-time algebraic Riccati equation.\n",
    "        \n",
    "Some notes:\n",
    "* The calculations of $(A,B,Q,R)$ are exact with high probability. \n",
    "* There are many ways to generate samples that would work besides the one chosen. \n",
    "* Enforcing the bound on $\\|x\\|$ is just for numerical stability. \n",
    "* The number of samples is chosen so that all of the required matrices have full rank with high probability. Specifically, $N$ is twice the dimension of the reshaped vector used to compute $Q$ and $R$. (More compact parameterizations that exploit the fact that there are no cross terms between $x$ and $u$ could also be used. This would then require fewer samples.)\n",
    "* A more complex \"model-free\" procedure is decribed after the problem. (You do not need to solve it.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to generate random systems for testing\n",
    "import numpy.random as rnd\n",
    "import scipy.stats as st\n",
    "\n",
    "def randomSystem():\n",
    "\n",
    "    n = rnd.randint(1,10)\n",
    "    p = rnd.randint(1,10)\n",
    "\n",
    "    A = rnd.randn(n,n)\n",
    "    B = rnd.randn(n,p)\n",
    "\n",
    "    Q = st.wishart.rvs(n+1,np.eye(n)).reshape((n,n))\n",
    "    R = st.wishart.rvs(p+1,np.eye(p)).reshape((p,p))\n",
    "    \n",
    "    return A,B,Q,R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "Code up the solution described above. Test it on several randomly generated systems. Print out $\\|K - K^*\\|$, where $K$ is the solution that you estimated, and $K^*$ is the true optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An alternative model free procedure\n",
    "\n",
    "Here we will just demonstrate an alternative \"model-free\" procedure (which does not compute $(A,B,Q,R)$). You don't need to solve anything here, but examining this code could be helpful for answering the question above.\n",
    "\n",
    "Say that $Q$ and $R$ are both positive definite. Then if the system is stabilizable, it computes a sequence of gains $K_i$  with the following properties:\n",
    "\n",
    "* $K_i$ is a stabilizing controller for all sufficiently large $i$.\n",
    "* The algorithm provides a certificate of stability  once a stabilizing controller is found.\n",
    "* $K_i$ converges to the optimum gain geometrically. \n",
    "    \n",
    "    \n",
    "The method works as follows:\n",
    "* Start with $K_i=0$ and discount factor $\\gamma_i = 0$.\n",
    "* For each $i\\ge 0$ do the following.\n",
    "    - Generate $N$ samples $(x_j,u_j,c_j,x_j^+)$ as described above. (Any $u_j$ sequence with sufficient noise injection for invertibility will work.)\n",
    "    - Solve for the action-value function, $Q_{i}(x,u)$ by least-squares, based on the system of equations $c_j = Q_{i}(x_j,u_j) - \\gamma_i Q_{i}(x_j^+,K_i x_j^+)$. \n",
    "    - Compute $P_i$ such that $x^\\top P_i x = Q_i(x,K_i x)$. This is the matrix corresponding to the value function.\n",
    "    - Let $\\gamma_{i+1} > \\gamma_i$  be chosen as follows:\n",
    "        - Let $\\hat Q_i$ be the action-value function computed from the equations $c_j = \\hat Q_{i}(x_j,u_j) - \\gamma_{i+1} \\hat Q_{i}(x_j^+,K_i x_j^+)$\n",
    "        - Let $\\hat P_i$ be the corresponding value function matrix\n",
    "        - Use bisection to find the largest value of $\\gamma_{i+1}$ so that\n",
    "            - $\\gamma_{i+1} \\le 1$\n",
    "            - $\\hat P_i$ is positive definite\n",
    "            - $\\|P_i - \\hat P_i \\|\\le b$, for some fixed upper bound $b$. \n",
    "           \n",
    "    - Let $K_{i+1}$ be the minimizing gain: $\\min_u \\hat Q_i(x,u) = \\hat Q_i(x,K_{i+1})$\n",
    "    \n",
    "    \n",
    "With this description, the properties of the algorithm can be described more precisely.\n",
    "* $\\gamma_i$ is monotonically increasing.\n",
    "* For all $i$, $K_i$ stabilizes the system $(\\sqrt{\\gamma_i} A,\\sqrt{\\gamma_i}B)$.\n",
    "* If $\\gamma_i = 1$, then $K_i$ is stabilizing. Furthermore, $\\gamma_j=1$ and $K_j$ is stabilizing for all $j\\ge i$.\n",
    "* If the system is stabilizable, then:\n",
    "    - $\\gamma_i=1$ after finitely many steps\n",
    "    - Once $\\gamma_i=1$, the steps become equivalent to policy iteration, and thus converge to the optimal gain.\n",
    "* If the system is not stabilizable,then $\\lim_{i\\to\\infty} \\gamma_i =\\hat\\gamma$ such that $(\\sqrt{\\hat \\gamma} A,\\sqrt{\\hat \\gamma }B)$ is not stabilizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1 , ||K-K*||: 4.387313883726907 , rho(A+BK): 2.4747052785311574 , gamma: 0.0\n",
      "Episode:  2 , ||K-K*||: 4.189920826003797 , rho(A+BK): 2.3259085586532704 , gamma: 0.015625\n",
      "Episode:  3 , ||K-K*||: 3.8910627105363433 , rho(A+BK): 2.243660611272209 , gamma: 0.04638671875\n",
      "Episode:  4 , ||K-K*||: 3.5842279277332767 , rho(A+BK): 2.1369336528785374 , gamma: 0.0761871337890625\n",
      "Episode:  5 , ||K-K*||: 3.2587851241767463 , rho(A+BK): 1.9873796510820385 , gamma: 0.1050562858581543\n",
      "Episode:  6 , ||K-K*||: 3.0992563876702235 , rho(A+BK): 1.912696244182855 , gamma: 0.11903978139162064\n",
      "Episode:  7 , ||K-K*||: 2.93648795321798 , rho(A+BK): 1.8261267624190938 , gamma: 0.13280478480737656\n",
      "Episode:  8 , ||K-K*||: 2.778012179160648 , rho(A+BK): 1.7401022926239602 , gamma: 0.1463547100447613\n",
      "Episode:  9 , ||K-K*||: 2.6258789030057277 , rho(A+BK): 1.6566135364295766 , gamma: 0.1596929177003119\n",
      "Episode:  10 , ||K-K*||: 2.4816948786530375 , rho(A+BK): 1.5772502302143596 , gamma: 0.17282271586124454\n",
      "Episode:  11 , ||K-K*||: 2.34636454689091 , rho(A+BK): 1.5028445536976984 , gamma: 0.1857473609259126\n",
      "Episode:  12 , ||K-K*||: 2.220225988842211 , rho(A+BK): 1.433706259607724 , gamma: 0.19847005841144522\n",
      "Episode:  13 , ||K-K*||: 2.1032037855370835 , rho(A+BK): 1.3698074704955878 , gamma: 0.2109939637487664\n",
      "Episode:  14 , ||K-K*||: 1.9949509359078776 , rho(A+BK): 1.310922902371321 , gamma: 0.22332218306519191\n",
      "Episode:  15 , ||K-K*||: 1.8949603580410876 , rho(A+BK): 1.2567247452326635 , gamma: 0.23545777395479828\n",
      "Episode:  16 , ||K-K*||: 1.8026448815025815 , rho(A+BK): 1.2068427259320782 , gamma: 0.24740374623675454\n",
      "Episode:  17 , ||K-K*||: 1.7173909769310194 , rho(A+BK): 1.1609000409126897 , gamma: 0.2591630627018052\n",
      "Episode:  18 , ||K-K*||: 1.6385927954792068 , rho(A+BK): 1.1185335620211512 , gamma: 0.2707386398470895\n",
      "Episode:  19 , ||K-K*||: 1.5656723490122635 , rho(A+BK): 1.0794042115546465 , gamma: 0.2821333485994787\n",
      "Episode:  20 , ||K-K*||: 1.498090357953366 , rho(A+BK): 1.0432014024320562 , gamma: 0.2933500150276118\n",
      "Episode:  21 , ||K-K*||: 1.43535104121422 , rho(A+BK): 1.0096440244818194 , gamma: 0.30439142104280537\n",
      "Episode:  22 , ||K-K*||: 1.377003117545481 , rho(A+BK): 0.9784795163026244 , gamma: 0.31526030508901154\n",
      "Episode:  23 , ||K-K*||: 1.322638541717656 , rho(A+BK): 0.9494819571356642 , gamma: 0.3259593628219958\n",
      "Episode:  24 , ||K-K*||: 1.2718899769115402 , rho(A+BK): 0.922449733214364 , gamma: 0.33649124777790207\n",
      "Episode:  25 , ||K-K*||: 1.2244276420744342 , rho(A+BK): 0.8972030981406748 , gamma: 0.3468585720313724\n",
      "Episode:  26 , ||K-K*||: 1.1799559357897285 , rho(A+BK): 0.8735818035823623 , gamma: 0.3570639068433822\n",
      "Episode:  27 , ||K-K*||: 1.138210077527638 , rho(A+BK): 0.8514428905176701 , gamma: 0.36710978329895433\n",
      "Episode:  28 , ||K-K*||: 1.0989529050320723 , rho(A+BK): 0.8306586804471111 , gamma: 0.3769986929349082\n",
      "Episode:  29 , ||K-K*||: 1.0619719006398787 , rho(A+BK): 0.811114976791835 , gamma: 0.38673308835780024\n",
      "Episode:  30 , ||K-K*||: 1.0270764781661916 , rho(A+BK): 0.7927094706009462 , gamma: 0.39631538385220966\n",
      "Episode:  31 , ||K-K*||: 0.9940955368160143 , rho(A+BK): 0.7753503364499471 , gamma: 0.40574795597951885\n",
      "Episode:  32 , ||K-K*||: 0.9628752738570783 , rho(A+BK): 0.7589550008116458 , gamma: 0.41503314416733883\n",
      "Episode:  33 , ||K-K*||: 0.9332772397085021 , rho(A+BK): 0.7434490642409864 , gamma: 0.42417325128972416\n",
      "Episode:  34 , ||K-K*||: 0.9051766152171015 , rho(A+BK): 0.7287653592459488 , gamma: 0.43317054423832224\n",
      "Episode:  35 , ||K-K*||: 0.878460689585379 , rho(A+BK): 0.7148431270033831 , gamma: 0.44202725448459845\n",
      "Episode:  36 , ||K-K*||: 0.8530275176690905 , rho(A+BK): 0.7016272976840878 , gamma: 0.4507455786332766\n",
      "Episode:  37 , ||K-K*||: 0.8287847364754724 , rho(A+BK): 0.6890678608512439 , gamma: 0.4593276789671316\n",
      "Episode:  38 , ||K-K*||: 0.8056485222545671 , rho(A+BK): 0.6771193140382611 , gamma: 0.4677756839832702\n",
      "Episode:  39 , ||K-K*||: 0.7835426713289334 , rho(A+BK): 0.6657401791349394 , gamma: 0.4760916889210316\n",
      "Episode:  40 , ||K-K*||: 0.7623977895802542 , rho(A+BK): 0.6548925775900071 , gamma: 0.4842777562816405\n",
      "Episode:  41 , ||K-K*||: 0.7421505772053711 , rho(A+BK): 0.6445418566506804 , gamma: 0.49233591633973983\n",
      "Episode:  42 , ||K-K*||: 0.722743196950756 , rho(A+BK): 0.6346562599310123 , gamma: 0.5002681676469314\n",
      "Episode:  43 , ||K-K*||: 0.7041227154651301 , rho(A+BK): 0.6252066365251312 , gamma: 0.5080764775274481\n",
      "Episode:  44 , ||K-K*||: 0.6862406087091525 , rho(A+BK): 0.6161661836800857 , gamma: 0.5157627825660818\n",
      "Episode:  45 , ||K-K*||: 0.6690523235037417 , rho(A+BK): 0.60751021873468 , gamma: 0.5233289890884867\n",
      "Episode:  46 , ||K-K*||: 0.6363209018902567 , rho(A+BK): 0.5910645567132938 , gamma: 0.5382249581794715\n",
      "Episode:  47 , ||K-K*||: 0.6059788512334311 , rho(A+BK): 0.5760100549889682 , gamma: 0.5526554282363629\n",
      "Episode:  48 , ||K-K*||: 0.5777410730655844 , rho(A+BK): 0.562116595430535 , gamma: 0.5666349461039766\n",
      "Episode:  49 , ||K-K*||: 0.5513907415772804 , rho(A+BK): 0.5492624704045875 , gamma: 0.5801776040382274\n",
      "Episode:  50 , ||K-K*||: 0.5267406329076881 , rho(A+BK): 0.5373419032044754 , gamma: 0.5932970539120328\n",
      "Episode:  51 , ||K-K*||: 0.5036285011977614 , rho(A+BK): 0.5262628303016021 , gamma: 0.6060065209772818\n",
      "Episode:  52 , ||K-K*||: 0.4819130325983711 , rho(A+BK): 0.5159447655301018 , gamma: 0.6183188171967418\n",
      "Episode:  53 , ||K-K*||: 0.4614705505410701 , rho(A+BK): 0.5063170493772704 , gamma: 0.6302463541593436\n",
      "Episode:  54 , ||K-K*||: 0.4421923163468306 , rho(A+BK): 0.4973174039620825 , gamma: 0.6418011555918641\n",
      "Episode:  55 , ||K-K*||: 0.42398230575082574 , rho(A+BK): 0.4888907337850077 , gamma: 0.6529948694796184\n",
      "Episode:  56 , ||K-K*||: 0.4067553679424837 , rho(A+BK): 0.48182663372813234 , gamma: 0.6638387798083802\n",
      "Episode:  57 , ||K-K*||: 0.3904356936779201 , rho(A+BK): 0.47652380129412086 , gamma: 0.6743438179393684\n",
      "Episode:  58 , ||K-K*||: 0.37495553444592244 , rho(A+BK): 0.4715052174356039 , gamma: 0.6845205736287632\n",
      "Episode:  59 , ||K-K*||: 0.36025412655324957 , rho(A+BK): 0.46675074898771707 , gamma: 0.6943793057028643\n",
      "Episode:  60 , ||K-K*||: 0.3462767833154029 , rho(A+BK): 0.46224211408869537 , gamma: 0.7039299523996498\n",
      "Episode:  61 , ||K-K*||: 0.33297412577470775 , rho(A+BK): 0.45796267197879076 , gamma: 0.7131821413871609\n",
      "Episode:  62 , ||K-K*||: 0.3203014281235803 , rho(A+BK): 0.4538972409807152 , gamma: 0.7221451994688121\n",
      "Episode:  63 , ||K-K*||: 0.30821805848918415 , rho(A+BK): 0.4500319403403516 , gamma: 0.7308281619854118\n",
      "Episode:  64 , ||K-K*||: 0.29668699936000065 , rho(A+BK): 0.4463540523462992 , gamma: 0.7392397819233677\n",
      "Episode:  65 , ||K-K*||: 0.2856744347817194 , rho(A+BK): 0.44285190174662703 , gamma: 0.7473885387382624\n",
      "Episode:  66 , ||K-K*||: 0.2751493937617983 , rho(A+BK): 0.4395147499771076 , gamma: 0.7552826469026916\n",
      "Episode:  67 , ||K-K*||: 0.2650834411652846 , rho(A+BK): 0.43633270210735936 , gamma: 0.7629300641869825\n",
      "Episode:  68 , ||K-K*||: 0.2554504088821911 , rho(A+BK): 0.43329662475703895 , gamma: 0.7703384996811393\n",
      "Episode:  69 , ||K-K*||: 0.24622616127922517 , rho(A+BK): 0.4303980734880814 , gamma: 0.7775154215661038\n",
      "Episode:  70 , ||K-K*||: 0.2373883899180501 , rho(A+BK): 0.4276292284189977 , gamma: 0.784468064642163\n",
      "Episode:  71 , ||K-K*||: 0.22891643335807244 , rho(A+BK): 0.4249828369909176 , gamma: 0.7912034376220953\n",
      "Episode:  72 , ||K-K*||: 0.22079111850880442 , rho(A+BK): 0.4224521629684211 , gamma: 0.7977283301964049\n",
      "Episode:  73 , ||K-K*||: 0.2129946205853067 , rho(A+BK): 0.42003094089478865 , gamma: 0.8040493198777672\n",
      "Episode:  74 , ||K-K*||: 0.20551033911775116 , rho(A+BK): 0.41771333532908655 , gamma: 0.810172778631587\n",
      "Episode:  75 , ||K-K*||: 0.19832278792257235 , rho(A+BK): 0.41549390428559424 , gamma: 0.8161048792993499\n",
      "Episode:  76 , ||K-K*||: 0.19141749717860781 , rho(A+BK): 0.4133675663784263 , gamma: 0.8218516018212453\n",
      "Episode:  77 , ||K-K*||: 0.17819252304917774 , rho(A+BK): 0.40930993430629714 , gamma: 0.8329858767074174\n",
      "Episode:  78 , ||K-K*||: 0.165978796776882 , rho(A+BK): 0.4055847757766388 , gamma: 0.8434242594132038\n",
      "Episode:  79 , ||K-K*||: 0.15468189550568345 , rho(A+BK): 0.40215697062198213 , gamma: 0.8532102431998785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  80 , ||K-K*||: 0.14422118041214135 , rho(A+BK): 0.3989985370974231 , gamma: 0.8623846029998861\n",
      "Episode:  81 , ||K-K*||: 0.13452489352599956 , rho(A+BK): 0.3960847018874084 , gamma: 0.8709855653123932\n",
      "Episode:  82 , ||K-K*||: 0.12552891369243116 , rho(A+BK): 0.3933934590480328 , gamma: 0.8790489674803686\n",
      "Episode:  83 , ||K-K*||: 0.1171757228939888 , rho(A+BK): 0.3909051947180118 , gamma: 0.8866084070128457\n",
      "Episode:  84 , ||K-K*||: 0.10941354502116563 , rho(A+BK): 0.38860237190446345 , gamma: 0.8936953815745429\n",
      "Episode:  85 , ||K-K*||: 0.10219562336942194 , rho(A+BK): 0.38646926433514167 , gamma: 0.9003394202261339\n",
      "Episode:  86 , ||K-K*||: 0.09547961060251234 , rho(A+BK): 0.38449173063071124 , gamma: 0.9065682064620005\n",
      "Episode:  87 , ||K-K*||: 0.08922705051696754 , rho(A+BK): 0.3826570217928961 , gamma: 0.9124076935581256\n",
      "Episode:  88 , ||K-K*||: 0.08340293524331564 , rho(A+BK): 0.38095361636894254 , gamma: 0.9178822127107427\n",
      "Episode:  89 , ||K-K*||: 0.07257685151625368 , rho(A+BK): 0.37780049778715347 , gamma: 0.9281469361218998\n",
      "Episode:  90 , ||K-K*||: 0.06320589123246328 , rho(A+BK): 0.37508808780140535 , gamma: 0.9371285691066624\n",
      "Episode:  91 , ||K-K*||: 0.05508045813323283 , rho(A+BK): 0.3727479529119856 , gamma: 0.9449874979683296\n",
      "Episode:  92 , ||K-K*||: 0.04802605054084651 , rho(A+BK): 0.37072525358356656 , gamma: 0.9518640607222884\n",
      "Episode:  93 , ||K-K*||: 0.041894891160384314 , rho(A+BK): 0.3689741298063757 , gamma: 0.9578810531320023\n",
      "Episode:  94 , ||K-K*||: 0.036561256478303474 , rho(A+BK): 0.3674560201737846 , gamma: 0.963145921490502\n",
      "Episode:  95 , ||K-K*||: 0.0272945287115039 , rho(A+BK): 0.3648291831462058 , gamma: 0.9723594411178765\n",
      "Episode:  96 , ||K-K*||: 0.020401942193110284 , rho(A+BK): 0.36288619877674716 , gamma: 0.9792695808384073\n",
      "Episode:  97 , ||K-K*||: 0.01526314083988196 , rho(A+BK): 0.361442959312428 , gamma: 0.9844521856288055\n",
      "Episode:  98 , ||K-K*||: 0.007602261698352986 , rho(A+BK): 0.3592992824029524 , gamma: 0.9922260928144028\n",
      "Episode:  99 , ||K-K*||: 4.651196338753539e-06 , rho(A+BK): 0.35718259385854495 , gamma: 1.0\n",
      "Episode:  100 , ||K-K*||: 1.0366395694184654e-11 , rho(A+BK): 0.35718364642668554 , gamma: 1.0\n"
     ]
    }
   ],
   "source": [
    "#######  Helper Code #######\n",
    "import scipy.linalg as la\n",
    "def trilStack(M):\n",
    "    n = len(M)\n",
    "    v = []\n",
    "    for i in range(n):\n",
    "        v.append(M[i:,i])\n",
    "    return np.hstack(v)\n",
    "\n",
    "def trilUnstack(v):\n",
    "    d = len(v)\n",
    "    n = int((np.sqrt(1+8*d)-1)/2)\n",
    "    M = np.zeros((n,n))\n",
    "    j = 0\n",
    "    for i in range(n):\n",
    "        M[i:,i] = v[j:j+n-i]\n",
    "        j = j+n-i\n",
    "    return M\n",
    "\n",
    "def symFromTrilStack(v):\n",
    "    L = trilUnstack(v)\n",
    "    D = np.diag(np.diag(L))\n",
    "    return L + L.T - D\n",
    "\n",
    "def quadraticMonomials(z):\n",
    "    M = np.outer(z,z)\n",
    "    M_L = np.tril(M,-1)\n",
    "    M_D = np.diag(np.diag(M))\n",
    "    return trilStack(2*M_L + M_D)\n",
    "\n",
    "def buildPhi(X,U,X_next,K,gamma):\n",
    "    Horizon = len(U)\n",
    "    Phi = []\n",
    "    for x,u,x_next in zip(X,U,X_next):\n",
    "        z = np.hstack([x,u])\n",
    "        z_next = np.hstack([x_next,K@x_next])\n",
    "        # Accounting for noise requires extra parameter\n",
    "        psi = np.hstack(quadraticMonomials(z))\n",
    "        psi_next = np.hstack(quadraticMonomials(z_next))\n",
    "        phi = psi - gamma * psi_next\n",
    "        Phi.append(phi)\n",
    "    return np.array(Phi)\n",
    "    \n",
    "    \n",
    "    \n",
    "def getVandQmatrices(X,U,X_next,c,K,gamma):\n",
    "    F = np.vstack([np.eye(n),K])\n",
    "    Phi = buildPhi(X,U,X_next,K,gamma)\n",
    "    theta = la.lstsq(Phi,c)[0]\n",
    "    # The first entry of theta is the noise variance\n",
    "    Q_mat = symFromTrilStack(theta)\n",
    "    P = F.T @ Q_mat @ F\n",
    "    return P,Q_mat\n",
    "    \n",
    "def increaseGamma(X,U,X_next,c,K,gamma):\n",
    "    LB = gamma \n",
    "    UB = 1.\n",
    "    \n",
    "    p,n = K.shape\n",
    "    \n",
    "    P_cur,_ = getVandQmatrices(X,U,X_next,c,K,gamma)\n",
    "    \n",
    "    minEig = -np.inf\n",
    "    gap = 20 \n",
    "    # I could fully bisect, but it isn't needed\n",
    "    while (minEig < 0) or (gap > 10):\n",
    "        gamma = UB\n",
    "        P,Q_mat = getVandQmatrices(X,U,X_next,c,K,gamma)\n",
    "        minEig = la.eigvalsh(P,eigvals=[0,0])[0]\n",
    "        gap = la.norm(P-P_cur)\n",
    "        \n",
    "        if (minEig < 0) or (gap > 10):\n",
    "            UB = .5 * (UB + LB)\n",
    "    return gamma, Q_mat, P\n",
    "    \n",
    "def genX(n,b):\n",
    "    r = rnd.rand() * b\n",
    "    x = rnd.randn(n)\n",
    "    return r * x / la.norm(x)\n",
    "\n",
    "#### Main Code ##### \n",
    "\n",
    "A,B,Q,R = randomSystem()\n",
    "n,p = B.shape\n",
    "\n",
    "NumEpisodes = 10\n",
    "x_bound = 100\n",
    "x = genX(n,x_bound)\n",
    "\n",
    "Horizon = (p+n)*(p+n+1)\n",
    "K = np.zeros((p,n))\n",
    "gamma = 0.\n",
    "\n",
    "\n",
    "P_opt = la.solve_discrete_are(A,B,Q,R)\n",
    "K_opt = -la.solve(R+B.T@P_opt@B,B.T@P_opt@A)\n",
    "\n",
    "K_err = 10\n",
    "\n",
    "ep =0\n",
    "while K_err > 1e-6 :\n",
    "    ep += 1\n",
    "    X = []\n",
    "    U = []\n",
    "    X_next = []\n",
    "    c = []\n",
    "    # Simulate the system\n",
    "    for t in range(Horizon):\n",
    "        u = K@x + .1 * rnd.randn(p)\n",
    "        x_next = A@x + B @ u \n",
    "        c.append(x@Q@x + u@R@u)\n",
    "        X.append(x)\n",
    "        U.append(u)\n",
    "        X_next.append(x_next)\n",
    "        if la.norm(x_next) <= x_bound:\n",
    "            x = x_next\n",
    "        else:\n",
    "            x = genX(n,x_bound)\n",
    "        \n",
    "    c = np.array(c)\n",
    "    \n",
    "    eigMax = np.max(np.abs(la.eigvals(A+B@K)))\n",
    "    K_err = la.norm(K-K_opt)\n",
    "    print('Episode: ',ep,', ||K-K*||:',K_err, ', rho(A+BK):', eigMax, ', gamma:',gamma)\n",
    "    gamma,Q_mat, P = increaseGamma(X,U,X_next,c,K,gamma)\n",
    "    # improve K\n",
    "    Omega = Q_mat[n:,n:]\n",
    "    Psi = Q_mat[n:,:n]\n",
    "    K = -la.solve(Omega,Psi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
